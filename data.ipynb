{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "from attrs import frozen\n",
    "from lxml import etree\n",
    "import feedparser\n",
    "from snscrape.modules.telegram import TelegramChannelScraper\n",
    "from facebook_scraper import get_posts as get_facebook_posts\n",
    "from ensta import Host as InstaClient\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@frozen\n",
    "class TextEntry:\n",
    "    author: str\n",
    "    text: str\n",
    "    date: datetime\n",
    "    source: str\n",
    "    source_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVALNY_BLOG_URL = r'https://navalny.com'\n",
    "NUM_PAGES_IN_BLOG = 298\n",
    "NAVALNY_TELEGRAM_NAME = r'navalny'\n",
    "NAVALNY_TWITTER_NAME = r'navalny'\n",
    "NAVALNY_FB_NAME = r'navalny'\n",
    "NAVALNY_INSTA_NAME = r'navalny'\n",
    "\n",
    "VOLKOV_LJ_URL = r'https://leonwolf.livejournal.com'\n",
    "VOLKOV_TELEGRAM_NAME = r'leonid_volkov'\n",
    "VOLKOV_TWITTER_NAME = r'leonidvolkov'\n",
    "VOLKOV_FB_NAME = r'leonid.m.volkov'\n",
    "VOLKOV_INSTA_NAME = r'leonidvolkov'\n",
    "\n",
    "SOBOL_LJ_URL = r'https://sobollubov.livejournal.com'\n",
    "SOBOL_TELEGRAM_NAME = r'TeamSobol'\n",
    "SOBOK_TWITTER_NAME = r'SobolLubov'\n",
    "SOBOL_FB_NAME = r'soboll.ru'\n",
    "SOBOL_INSTA_NAME = r'sobollubov'\n",
    "\n",
    "PEVCHIKH_TWITTER_NAME = r'pevchikh'\n",
    "PEVCHIKH_INSTA_NAME = r'maria_pevchikh'\n",
    "\n",
    "ALBUROV_FB_NAME = r'alburov'\n",
    "ALBUROV_TWITTER_NAME = r'alburov'\n",
    "ALBUROV_INSTA_NAME = r'alburov'\n",
    "\n",
    "ZHDANOV_TELEGRAM_NAME = r'ioannzh'\n",
    "ZHDANOV_TWITTER_NAME = r'ioannZH'\n",
    "ZHDANOV_FB_NAME = r'zhdanovivan'\n",
    "ZHDANOV_INSTA_NAME = r'ioannzh'\n",
    "\n",
    "YARMYSH_TELEGRAM_NAME = r'Kira_Yarmysh'\n",
    "YARMYSH_TWITTER_NAME = r'Kira_Yarmysh'\n",
    "YARMYSH_INSTA_NAME = r'kira_yarmysh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./credentials/lj_cookies.json') as file:\n",
    "    lj_cookies_json = json.load(file)\n",
    "    lj_cookies = requests.cookies.RequestsCookieJar()\n",
    "\n",
    "    for cookie in lj_cookies_json:\n",
    "        lj_cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'], path=cookie['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credentials/insta_credentials.json') as file:\n",
    "    credentials = json.load(file)\n",
    "    insta_username = credentials['username']\n",
    "    insta_password = credentials['password']\n",
    "\n",
    "insta_client = InstaClient(insta_username, insta_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lj_page(url: str, num_attempts: int = 10) -> bs4.BeautifulSoup:\n",
    "    for _ in range(num_attempts):\n",
    "        response = requests.get(url, cookies=lj_cookies)\n",
    "        if response.status_code == 200:\n",
    "            return bs4.BeautifulSoup(response.text)\n",
    "        time.sleep(0.2)\n",
    "    raise RuntimeError(f'Failed to get {url}')\n",
    "\n",
    "\n",
    "def iter_lj_articles(lj_url: str) -> Iterable[bs4.element.Tag]:\n",
    "    next_url = lj_url\n",
    "    while True:\n",
    "        page = get_lj_page(next_url)\n",
    "        articles = page.find_all('article')\n",
    "        articles = [article for article in articles if 'j-e-no-entries-message' not in article.get('class', [])]\n",
    "        if not articles:\n",
    "            break\n",
    "        yield from articles\n",
    "        next_link_element = page.find('li', class_='j-nav-item j-page-nav-item j-page-nav-item-prev')\n",
    "        if not next_link_element:\n",
    "            break\n",
    "        next_url = next_link_element.find('a').get('href')\n",
    "\n",
    "\n",
    "def parse_lj(author: str, lj_url: str) -> List[TextEntry]:\n",
    "    result = []\n",
    "\n",
    "    articles = iter_lj_articles(lj_url)\n",
    "    for article in tqdm(articles, desc=f'Parsing {lj_url}'):\n",
    "        date_elem = article.find('time')\n",
    "        if not date_elem:\n",
    "            continue\n",
    "        date_str = date_elem.get('datetime')\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        title_elem = article.find('h3', class_='entryunit__title')\n",
    "        post_url = title_elem.find('a').get('href')\n",
    "        text = article.find('div', class_='entryunit__text').text\n",
    "        result.append(TextEntry(author=author, date=date, source=post_url, source_type=lj_url, text=text))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_xml(xml_string: str) -> str:\n",
    "    build_text_list = etree.XPath(\"//text()\")\n",
    "    tree = etree.fromstring(xml_string, etree.HTMLParser())\n",
    "    return \"\\n\".join(build_text_list(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feed(author: str, source_type: str, url: str) -> List[TextEntry]:\n",
    "    feed = feedparser.parse(url)\n",
    "    result = []\n",
    "\n",
    "    for item in feed['entries']:\n",
    "        source = item['link']\n",
    "        date = parse_date(item['published'])\n",
    "        title = item['title']\n",
    "        text_body = clean_xml(item['description'])\n",
    "        text = f'{title}\\n{text_body}'\n",
    "        result.append(TextEntry(author=author, source=source, source_type=source_type, date=date, text=text))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_telegram_channel(author: str, channel_name: str) -> List[TextEntry]:\n",
    "    scraper = TelegramChannelScraper(channel_name)\n",
    "    result = []\n",
    "    for post in tqdm(scraper.get_items(), desc=f'Parsing telegram {channel_name}'):\n",
    "        source = post.url\n",
    "        text = post.content\n",
    "        if not text:\n",
    "            continue\n",
    "        source_type = f't.me/{channel_name}'\n",
    "        date = post.date\n",
    "        result.append(TextEntry(author=author, source=source, text=text, source_type=source_type, date=date))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fb_feed(author: str, username: str, cookie_path: str = './credentials/fb_cookies.json', pages: int = 100) -> List[TextEntry]:\n",
    "    posts = get_facebook_posts(username, cookies=cookie_path, pages=pages, options={\"posts_per_page\": 200})\n",
    "    result = []\n",
    "    for post in tqdm(posts, desc=f'Parsing facebook {username}'):\n",
    "        date = post['time']\n",
    "        text = post['post_text']\n",
    "        if not text:\n",
    "            continue\n",
    "        source = post['post_url']\n",
    "        source_type = f'facebook.com/{username}'\n",
    "        result.append(TextEntry(author=author, date=date, text=text, source=source, source_type=source_type))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_insta_feed(author: str, username: str, client: InstaClient) -> List[TextEntry]:\n",
    "    posts = client.posts(username)\n",
    "    result = []\n",
    "    for post in tqdm(posts, desc=f'Parsing instagram {username}'):\n",
    "        if post is None:\n",
    "            continue\n",
    "        text = post.caption_text\n",
    "        if not text or post.user.username != username:\n",
    "            continue\n",
    "        source = post.share_url\n",
    "        source_type = f'instagram.com/{username}'\n",
    "        date = datetime.fromtimestamp(post.taken_at)\n",
    "        result.append(TextEntry(author=author, text=text, source=source, source_type=source_type, date=date))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navalny_blog = []\n",
    "\n",
    "for page_no in tqdm(range(1, NUM_PAGES_IN_BLOG+1), desc='Parsing Navalny blog'):\n",
    "    response = requests.get(f'{NAVALNY_BLOG_URL}?p={page_no}')\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "    posts = soup.find_all('div', class_='b-post')\n",
    "    for post in posts:\n",
    "        title_element = post.find('h2', class_='b-title')\n",
    "        link = title_element.find('a').get('href')\n",
    "        post_url = f'{NAVALNY_BLOG_URL}{link}'\n",
    "        text = post.find('div', class_='b-post__content').text.strip()\n",
    "        date_str = post.find('div', class_='b-post__info__item').text.strip()\n",
    "        date = datetime.strptime(date_str, '%d.%m.%Y, %H:%M')\n",
    "        navalny_blog.append(TextEntry(author='Navalny', text=text, source=post_url, source_type=NAVALNY_BLOG_URL, date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navalny_telegram = parse_telegram_channel('Navalny', NAVALNY_TELEGRAM_NAME)\n",
    "navalny_fb = parse_fb_feed('Navalny', NAVALNY_FB_NAME)\n",
    "navalny_insta = parse_insta_feed('Navalny', NAVALNY_INSTA_NAME, insta_client)\n",
    "\n",
    "volkov_lj = parse_lj('Volkov', VOLKOV_LJ_URL)\n",
    "volkov_telegram = parse_telegram_channel('Volkov', VOLKOV_TELEGRAM_NAME)\n",
    "volkov_fb = parse_fb_feed('Volkov', VOLKOV_FB_NAME)\n",
    "volkov_insta = parse_insta_feed('Volkov', VOLKOV_INSTA_NAME, insta_client)\n",
    "\n",
    "sobol_lj = parse_lj('Sobol', SOBOL_LJ_URL)\n",
    "sobol_telegram = parse_telegram_channel('Sobol', SOBOL_TELEGRAM_NAME)\n",
    "sobol_fb = parse_fb_feed('Sobol', SOBOL_FB_NAME)\n",
    "sobol_insta = parse_insta_feed('Sobol', SOBOL_INSTA_NAME, insta_client)\n",
    "\n",
    "pevchikh_insta = parse_insta_feed('Pevchikh', PEVCHIKH_INSTA_NAME, insta_client)\n",
    "\n",
    "alburov_fb = parse_fb_feed('Alburov', ALBUROV_FB_NAME)\n",
    "alburov_insta = parse_insta_feed('Alburov', ALBUROV_INSTA_NAME, insta_client)\n",
    "\n",
    "zhdanov_telegram = parse_telegram_channel('Zhdanov', ZHDANOV_TELEGRAM_NAME)\n",
    "zhdanov_fb = parse_fb_feed('Zhdanov', ZHDANOV_FB_NAME)\n",
    "zhdanov_insta = parse_insta_feed('Zhdanov', ZHDANOV_INSTA_NAME, insta_client)\n",
    "\n",
    "yarmysh_telegram = parse_telegram_channel('Yarmysh', YARMYSH_TELEGRAM_NAME)\n",
    "yarmysh_insta = parse_insta_feed('Yarmysh', YARMYSH_INSTA_NAME, insta_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = sum([\n",
    "    navalny_blog,\n",
    "    navalny_telegram,\n",
    "    navalny_fb,\n",
    "    navalny_insta,\n",
    "    \n",
    "    volkov_lj,\n",
    "    volkov_telegram,\n",
    "    volkov_fb,\n",
    "    volkov_insta,\n",
    "\n",
    "    sobol_lj,\n",
    "    sobol_telegram,\n",
    "    sobol_fb,\n",
    "    sobol_insta,\n",
    "\n",
    "    pevchikh_insta,\n",
    "\n",
    "    alburov_fb,\n",
    "    alburov_insta,\n",
    "\n",
    "    zhdanov_telegram,\n",
    "    zhdanov_fb,\n",
    "    zhdanov_insta,\n",
    "\n",
    "    yarmysh_telegram,\n",
    "    yarmysh_insta,\n",
    "], start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fbk_archive.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['author', 'text', 'date', 'url', 'source_type'])\n",
    "    for text in all_texts:\n",
    "        writer.writerow(dict(\n",
    "            author=text.author,\n",
    "            text=text.text,\n",
    "            date=text.date.strftime('%Y-%m-%d'),\n",
    "            url=text.source,\n",
    "            source_type=text.source_type\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
